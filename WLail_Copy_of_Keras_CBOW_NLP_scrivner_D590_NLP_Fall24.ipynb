{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wlail-iu/D590-NLP-F24/blob/main/WLail_Copy_of_Keras_CBOW_NLP_scrivner_D590_NLP_Fall24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhoAVNSXL3gA"
      },
      "source": [
        "# Python Keras Handout - CBOW Model\n",
        "\n",
        "Fall 2023\n",
        "Olga Scrivner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXfvN1qIUluI"
      },
      "source": [
        "**CBOW model** predicts the _current word_ given context words within _specific window_.\n",
        "\n",
        "     - The input layer contains the context words\n",
        "     - The hidden layer contains the number of dimensions in which\n",
        "        we want to represent the current word presented at the output layer\n",
        "     - The output layer contains the current word\n",
        "\n",
        "\n",
        "![Mikolov et al., 2013](https://static.packt-cdn.com/products/9781786465825/graphics/B05525_03_05.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-FqRZUzwRRb"
      },
      "source": [
        "There are actually three frameworks to build CBOW model:\n",
        "\n",
        "### **tensoflow**\n",
        "- developed by Google researchers\n",
        "- designed to be used with python or javascript\n",
        "- python version can be installed via PYPI (pip)\n",
        "    - see documentation (license, release) - https://pypi.org/project/tensorflow/\n",
        "\n",
        "### **keras**\n",
        "\n",
        "- a neural network library (using tensoflow and theanos functionalities)\n",
        "- developed by a Google engineer Fran√ßois Chollet\n",
        "- designed to make implementing deep learning models as fast and easy as possible for research and development\n",
        "- requires tensoflow or theanos installation\n",
        "    - keras library can be installed via PYPI (pip)\n",
        "- see documentation (license, release) - https://pypi.org/project/keras/\n",
        "\n",
        "### **pythorch**\n",
        "\n",
        "- developed by facebook\n",
        "- allows for dynamic models deep learning\n",
        "        -  tensorflow/keras build static models\n",
        "        -  models have to be built from the beginning and then re-used\n",
        "    - can be installed via PYPI (pip)\n",
        "- see documentation - https://pypi.org/project/torch/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1odhy-igMBRs"
      },
      "source": [
        "To build CBOW model we will be using a Python interface to Keras with tensorflow serving as a \"backend engine\".\n",
        "- Keras is a Python library for developing and evaluating deep learning models\n",
        "- TensorFlow is an open-source symbolic tensor manipulation framework developed by Google: [link](http://www.tensorflow.org/)\n",
        "- Keras uses the TensorFlow backend by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIwHSoj4L2UZ"
      },
      "source": [
        "import keras.backend as K # K for Keras, just a common abbreviation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq8f9zjpNVrs"
      },
      "source": [
        "**Sequential model** is used when you have One Input and One output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyRM79gqNWPH"
      },
      "source": [
        "from keras.models import Sequential\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiHO6tfbNxpp"
      },
      "source": [
        "\n",
        "Keras provides pre-built layers for different neural network architecture.\n",
        "- **Dense**\n",
        "    - _Dense Layer_ is used for creating a deeply connected layer where each of the neurons of the dense layers receives input from all neurons of the previous layer.\n",
        "- **Embedding**\n",
        "   - _Embedding layer_ is a compression of the output and is used to embed higher dimensional data into lower dimensional vector space\n",
        "- **Lambda**\n",
        "   - _Lambda Layer_ is used for _transforming_ input using an expression or function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvP4tM27Nx_b"
      },
      "source": [
        "from keras.layers import Dense, Embedding, Lambda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6t-5tHbPlax"
      },
      "source": [
        "1. First you create a sequential constructor (similar to CountVectorizer() etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvWCbddwPln7"
      },
      "source": [
        "cbow = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3taTtw3sqIN"
      },
      "source": [
        "2. Then you can add layers to Sequential Contstructor using .add() and compile model with .compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9cLks8cQA67"
      },
      "source": [
        "## LAYER 1: Embedding Layer\n",
        "\n",
        "The Embedding layer generate weights. The output is a 2D vector with one embedding for each word .\n",
        "\n",
        "Embedding layer requires several arguments:\n",
        "\n",
        "- **input_dim**: We need to know $\\color{red}{vocabulary \\: size}$. `input_dim` should equal size of vocabulary + 1\n",
        "- **output_dim**: We need to select a $\\color{red}{dimension \\: size}$ for the dense embedding. This is the size of the vector space into which words will be embedded.\n",
        "       - when the layer is smaller , you compress more and lose more data.\n",
        "       - When the layer is bigger, you compress less and potentially overfit your input dataset to this layer.\n",
        "       - The larger vocabulary you have you want better representation of it - make the layer larger.\n",
        "       - If you have very sparse documents relatively to the vocabulary, then you want to \"get rid\" of unnecessary and\n",
        "          noisy words - you should compress more - make the embedding smaller.\n",
        "\n",
        "- **input_length**: Length of input sequences, when it is constant. In CBOW model, we use $\\color{red}{size \\: of \\: window*2}$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PChF-ylfuZ0"
      },
      "source": [
        "To generate `input_dim`, we need to build a vocabulary maping:\n",
        "- extract each unique word from corpus vocabulary and map a unique numeric identifier to it.\n",
        "\n",
        "Keras provides a utility for text preprocesssing `from keras.preprocessing import text` ([see description of all functionalities](https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py)).\n",
        "- `Tokenizer()` is a class with several methods:\n",
        "    - `fit_on_texts`  - updates internal vocabulary based on a list of texts\n",
        "    - `word_index` - create a dictionary with words and their indices:\n",
        "           dict(zip(sorted_voc, list(range(1, len(sorted_voc) + 1)))) # from documentation (see link above)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClyNHj-r6bic"
      },
      "source": [
        "We are going to create a small corpus for illustration purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtWG-Gxl582Z",
        "outputId": "b268d254-412c-4f4a-b053-7f452c465133"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fArQ9TLsnZX"
      },
      "source": [
        "# Using a small example\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "corpus = ['The sky is blue and beautiful.',\n",
        "          'Love this blue and beautiful sky!',\n",
        "          'The quick brown fox jumps over the lazy dog.',\n",
        "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
        "          'I love green eggs, ham, sausages and bacon!',\n",
        "          'The brown fox is quick and the blue dog is lazy!',\n",
        "          'The sky is very blue and the sky is very beautiful today',\n",
        "          'The dog is lazy but the brown fox is quick!'\n",
        "]\n",
        "corpus = np.array(corpus)\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = wpt.tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "norm_corpus = normalize_corpus(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_9ro00s8G0m",
        "outputId": "92d4b793-64ca-4909-de1c-413af968e9f2"
      },
      "source": [
        "norm_corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['sky blue beautiful', 'love blue beautiful sky',\n",
              "       'quick brown fox jumps lazy dog',\n",
              "       'kings breakfast sausages ham bacon eggs toast beans',\n",
              "       'love green eggs ham sausages bacon',\n",
              "       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n",
              "       'dog lazy brown fox quick'], dtype='<U51')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuwG6mcosoiI"
      },
      "source": [
        "We will now import necessary keras library to generate vocabulary mapping for CBOW."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOQzbQTfrdbB"
      },
      "source": [
        "from keras.preprocessing import text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiuTdJhq7EQB"
      },
      "source": [
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(norm_corpus)\n",
        "word2id = tokenizer.word_index\n",
        "word2id['PAD'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS1K2nqq7exJ"
      },
      "source": [
        "Note: word2id creates a map of word and its index (unique identifier). Index always start with 1. \"sky\" is the first token after preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uNpPhIQ7aok",
        "outputId": "e8fafa89-2e37-445b-9b1d-faaf4dc794f1"
      },
      "source": [
        "word2id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sky': 1,\n",
              " 'blue': 2,\n",
              " 'beautiful': 3,\n",
              " 'quick': 4,\n",
              " 'brown': 5,\n",
              " 'fox': 6,\n",
              " 'lazy': 7,\n",
              " 'dog': 8,\n",
              " 'love': 9,\n",
              " 'sausages': 10,\n",
              " 'ham': 11,\n",
              " 'bacon': 12,\n",
              " 'eggs': 13,\n",
              " 'jumps': 14,\n",
              " 'kings': 15,\n",
              " 'breakfast': 16,\n",
              " 'toast': 17,\n",
              " 'beans': 18,\n",
              " 'green': 19,\n",
              " 'today': 20,\n",
              " 'PAD': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r95gfElbHYzo"
      },
      "source": [
        "We also need to create a reverse mapping from Index to Word `id2word` and word mapping to their sentences `wids`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7mWOSk5HKTT",
        "outputId": "3892d7d4-80bb-4e32-f376-38c2728abb0e"
      },
      "source": [
        "vocab_size = len(word2id)\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_corpus]\n",
        "print(\"Vocabulary size\", vocab_size)\n",
        "print(\"Index to Word\")\n",
        "print(id2word)\n",
        "print(\"Index to Sentence\")\n",
        "print(wids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size 21\n",
            "Index to Word\n",
            "{1: 'sky', 2: 'blue', 3: 'beautiful', 4: 'quick', 5: 'brown', 6: 'fox', 7: 'lazy', 8: 'dog', 9: 'love', 10: 'sausages', 11: 'ham', 12: 'bacon', 13: 'eggs', 14: 'jumps', 15: 'kings', 16: 'breakfast', 17: 'toast', 18: 'beans', 19: 'green', 20: 'today', 0: 'PAD'}\n",
            "Index to Sentence\n",
            "[[1, 2, 3], [9, 2, 3, 1], [4, 5, 6, 14, 7, 8], [15, 16, 10, 11, 12, 13, 17, 18], [9, 19, 13, 11, 10, 12], [5, 6, 4, 2, 8, 7], [1, 2, 1, 3, 20], [8, 7, 5, 6, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbuVMwhU8dYD"
      },
      "source": [
        "Padding is typically used to pad context words to a fixed length if needed. Most neural networks require the input sequence data with **the same length** so we need **padding**: to truncate or pad sequence (pad with 0s) into the same length.\n",
        "\n",
        "        - Notice that Index starts always with \"1\" because \"0\" is reserved for padding.\n",
        " Figure  below shows how zeros are added to shorter sequences to keep the same length for all sequences (sentences) and the first sequences is truncated aat the beginning.     \n",
        "\n",
        "![](https://miro.medium.com/max/1750/1*CPLhZoVSTCWgAxe2LKXoOA.png)\n",
        "source: https://towardsdatascience.com/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMXVqUNI81Be"
      },
      "source": [
        "`pad_sequence()` function from keras.preprocessing.sequence allows to add padding at the beggining (padding='pre') or at the end of the sequence (padding = 'post'). If the maximum length (`maxlen`) is not defined, it will take the size of the longuest sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2CfZw6AHwxz"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.utils import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qaH3leO4xo3",
        "outputId": "4ddd2902-3a80-4728-b34c-04e3a7669aeb"
      },
      "source": [
        "# max_length = 4 # you can try changing maximum length for padding\n",
        "padded_docs = pad_sequences(wids,  padding='pre') # maxlen=max_length,\n",
        "print(padded_docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  0  1  2  3]\n",
            " [ 0  0  0  0  9  2  3  1]\n",
            " [ 0  0  4  5  6 14  7  8]\n",
            " [15 16 10 11 12 13 17 18]\n",
            " [ 0  0  9 19 13 11 10 12]\n",
            " [ 0  0  5  6  4  2  8  7]\n",
            " [ 0  0  0  1  2  1  3 20]\n",
            " [ 0  0  0  8  7  5  6  4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Go_9i8sc2D"
      },
      "source": [
        "Finally we need to import keras utility helper function `from keras.utils import np_utils` (numpy related utility [link]{https://www.kite.com/python/docs/keras.utils.np_utils})"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kvuYAI4siwV"
      },
      "source": [
        "from keras.utils import np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDRnkHwM64E5"
      },
      "source": [
        "- We will be choosing our context window size = 2. Window size depends how many words on the left and right we want to include.\n",
        "- We will choose embedding size = 100. Our matrix will be vocabulary size x embeddeding size. This will be ouur output dimension (read ta the top about how to choose embedding size = `output_dim`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgRdNeqgNYWz"
      },
      "source": [
        "embed_size = 100\n",
        "window_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99Q-XWHFpWJD"
      },
      "source": [
        "The embedding layer is added to Sequential() Constructor with the following parameters:\n",
        "- input dimension = vocabulary size (20),\n",
        "- output ddimension = embedding size (100), a hyper-parameter\n",
        "- input length = window context (2*2 = 4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn0O0w-nQBKn"
      },
      "source": [
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkiuqTvRxEic"
      },
      "source": [
        "- **Input Shape** = 4 x 100 (4,  100).\n",
        "- **Output shape** = (None, 4, 100), where the first value represent **batch** size.\n",
        "    - `None` means the size/dimension is variable but each sample in the batch will always have a shape 4x100. So output is (None, 4,  100)\n",
        "    - If `batch size` is defined, then it is the subset size of your training sample (e.g. 100 out of 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11qlyxPaqh81"
      },
      "source": [
        "## LAYER 2: Lambda Layer\n",
        "\n",
        "Lambda is a wrapper for your function. For example, if you wanted to build a layer that squares its input, you could state:\n",
        "\n",
        "`model.add(lambda(lambda x: x ** 2))`\n",
        "\n",
        "In our case, Lambda layer will return the average of weights `mean`. Axis value specifies the axis along which the means are computed: **1** = along the column. The output dimention will be 100 (1-dimension).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kkKWhhH1SbS"
      },
      "source": [
        "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmGDqmysDXT9"
      },
      "source": [
        "## LAYER 3: Dense Layer\n",
        "\n",
        "Dense layer is a neural network layer that is fully connected:  each neuron in the dense layer receives input from all neurons of its previous layer\n",
        "\n",
        "- **unit** is the output size (just a positive integer)\n",
        "    - we use vocab size = 20 as output size\n",
        "- **activation** represents the activation function (e.g. softmax, sigmoid, ReLU)\n",
        "    - we use `softmax` which converts a vector of values to a probability distribution: output vector values are in range (0, 1) and sum to 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwPTfehV_6Wv"
      },
      "source": [
        "cbow.add(Dense(vocab_size, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drlz8yLMIiCN"
      },
      "source": [
        "### Compile Method\n",
        "\n",
        "Compile method configures the learning process. For the list of available optimizers [see link](https://faroit.com/keras-docs/1.1.0/optimizers/)\n",
        "\n",
        "- **RMSprop Optimizer**: RMSprop is a gradient-based optimization technique used in training neural networks. This normalization balances the step size (momentum), decreasing the step for large gradients to avoid exploding and increasing the step for small gradients to avoid vanishing [see link](https://medium.com/analytics-vidhya/a-complete-guide-to-adam-and-rmsprop-optimizer-75f4502d83be)\n",
        "\n",
        "- **Categorical Crossentropy** is used to compute the crossentropy loss between the labels and predictions.\n",
        "   - Crossentropy is a measure of the difference between two probability distributions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8c97Juj_9oC"
      },
      "source": [
        "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hO8I61otbHs"
      },
      "source": [
        "Using `model.summary()` we can see an overview of the model architecture.\n",
        "\n",
        "Each layer has an output and its shape is shown in the ‚ÄúOutput Shape‚Äù column. Each layer‚Äôs output becomes the input for the subsequent layer.\n",
        "\n",
        "The ‚ÄúParam #‚Äù column shows you the number of parameters that are trained for each layer.\n",
        "\n",
        "Initial input is 1 x Vocabulary (a vector of size 21)\n",
        "\n",
        "- Embedding Layer\n",
        "  - The input layer will take a 2-D matrix of shape (None, 21) which means that each sample must be reshaped into a vector of 21 elements.\n",
        "  - The output layer will be 3-D matrix of shape (None, 4, 100)\n",
        "  - The number of parameters will be 21 x 100 = 2100\n",
        "- Lambda Layer (averaging weights)\n",
        "  - The input will take the output from Embedding layer (3-D matrix)\n",
        "  - The output is 2-D matrix (None, 100), each batch sample must be shaped into a vector of 100 elements. Since Lambda is simply wrapping the function, there is no parameters.\n",
        "- Dense Layer\n",
        "  - The input is 2-D matrix from Lambda layer\n",
        "  - The output is 2-D matrix (None, 21) with sample batches as a vector of 20 elements (vocabulary size)\n",
        "  - The number of parameters = 100x21 plus number of biases for each node = 2121"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cc-oYlDAAi9",
        "outputId": "25cb0f2e-daea-4353-f4bd-73f374a59e77"
      },
      "source": [
        "print(cbow.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 4, 100)            2100      \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 100)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 21)                2121      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,221\n",
            "Trainable params: 4,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nBjP8XCbfJr",
        "outputId": "d0ddc2c1-b38b-48b2-e829-061ac6302b1e"
      },
      "source": [
        "for layer in cbow.layers:\n",
        "  print(layer.name, layer.input_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding (None, 4)\n",
            "lambda (None, 4, 100)\n",
            "dense (None, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9rYBcVmcNJK"
      },
      "source": [
        "## Generating Context pairs for CBOW Model\n",
        "\n",
        "Our function uses np_utils to convert vectors back to input matrix and sequence.pad_sequence to add zeros if the context length < 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ-7gUi81EUx"
      },
      "source": [
        "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "    context_length = window_size*2\n",
        "    for words in corpus:\n",
        "        sentence_length = len(words)\n",
        "        #print(words)\n",
        "        for index, word in enumerate(words):\n",
        "            #print(index,word)\n",
        "            context_words = []\n",
        "            label_word   = []\n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "\n",
        "            context_words.append([words[i]\n",
        "                                 for i in range(start, end)\n",
        "                                 if 0 <= i < sentence_length\n",
        "                                 and i != index])\n",
        "            label_word.append(word)\n",
        "            #print(word)\n",
        "            x = pad_sequences(context_words, maxlen=context_length)\n",
        "            y = np_utils.to_categorical(label_word, vocab_size)\n",
        "            yield (x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rfgTWJN1LV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d94b27e-5da5-4dc8-a103-0ae988c5774c"
      },
      "source": [
        "i = 0\n",
        "for x, y in generate_context_word_pairs(corpus=padded_docs, window_size=window_size, vocab_size=vocab_size):\n",
        "    print(x,y)\n",
        "    if 0 not in x[0]:\n",
        "        print(np.argwhere(y[0])[0][0])\n",
        "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
        "        #print(id2word[np.argwhere(y[0])[0][0]])\n",
        "        if i == 10:\n",
        "            break\n",
        "        i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 0 0]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 0 0]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 0 1]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 1 2]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 2 3]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 1 3]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 1 2]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 0 0]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 0 0]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 0 9]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 9 2]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 2 3]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 9 3 1]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 9 2 1]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 2 3]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 0 4]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 4 5]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 5 6]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[ 0  4  6 14]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[ 4  5 14  7]] [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "6\n",
            "Context (X): ['quick', 'brown', 'jumps', 'lazy'] -> Target (Y): fox\n",
            "[[5 6 7 8]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "14\n",
            "Context (X): ['brown', 'fox', 'lazy', 'dog'] -> Target (Y): jumps\n",
            "[[ 0  6 14  8]] [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[ 0  0 14  7]] [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[ 0  0 16 10]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "[[ 0 15 10 11]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "[[15 16 11 12]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "10\n",
            "Context (X): ['kings', 'breakfast', 'ham', 'bacon'] -> Target (Y): sausages\n",
            "[[16 10 12 13]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "11\n",
            "Context (X): ['breakfast', 'sausages', 'bacon', 'eggs'] -> Target (Y): ham\n",
            "[[10 11 13 17]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "12\n",
            "Context (X): ['sausages', 'ham', 'eggs', 'toast'] -> Target (Y): bacon\n",
            "[[11 12 17 18]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "13\n",
            "Context (X): ['ham', 'bacon', 'toast', 'beans'] -> Target (Y): eggs\n",
            "[[ 0 12 13 18]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "[[ 0  0 13 17]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "[[0 0 0 9]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[ 0  0  9 19]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[ 0  0 19 13]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[ 0  9 13 11]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "[[ 9 19 11 10]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "13\n",
            "Context (X): ['love', 'green', 'ham', 'sausages'] -> Target (Y): eggs\n",
            "[[19 13 10 12]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "11\n",
            "Context (X): ['green', 'eggs', 'sausages', 'bacon'] -> Target (Y): ham\n",
            "[[ 0 13 11 12]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[ 0  0 11 10]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 0 5]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 5 6]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 6 4]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 5 4 2]] [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[5 6 2 8]] [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "4\n",
            "Context (X): ['brown', 'fox', 'blue', 'dog'] -> Target (Y): quick\n",
            "[[6 4 8 7]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "2\n",
            "Context (X): ['fox', 'quick', 'dog', 'lazy'] -> Target (Y): blue\n",
            "[[0 4 2 7]] [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 2 8]] [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 0 0]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 0 1]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 1 2]] [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 0 2 1]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0 1 1 3]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[ 1  2  3 20]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "1\n",
            "Context (X): ['sky', 'blue', 'beautiful', 'today'] -> Target (Y): sky\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "DGhlFtD0ccXu",
        "outputId": "b2496dda-9167-4e46-eefd-a699ee039a2a"
      },
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "#display(SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False,\n",
        "   #              rankdir='TB', dpi=65).create(prog='dot', format='svg'))) # use dpi to reduce size of the image\n",
        "\n",
        "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False,\n",
        "rankdir='TB', dpi=65).create(prog='dot', format='svg'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"217pt\" height=\"274pt\" viewBox=\"0.00 0.00 240.00 304.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1.11 1.11) rotate(0) translate(4 300)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-300 236,-300 236,4 -4,4\"/>\n<!-- 140507752977504 -->\n<g id=\"node1\" class=\"node\">\n<title>140507752977504</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"11,-249.5 11,-295.5 221,-295.5 221,-249.5 11,-249.5\"/>\n<text text-anchor=\"middle\" x=\"49.5\" y=\"-268.8\" font-family=\"Times,serif\" font-size=\"14.00\">InputLayer</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"88,-249.5 88,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"115.5\" y=\"-280.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"88,-272.5 143,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"115.5\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"143,-249.5 143,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"182\" y=\"-280.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 4)]</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"143,-272.5 221,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"182\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 4)]</text>\n</g>\n<!-- 140507752977792 -->\n<g id=\"node2\" class=\"node\">\n<title>140507752977792</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-166.5 0,-212.5 232,-212.5 232,-166.5 0,-166.5\"/>\n<text text-anchor=\"middle\" x=\"40\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\">Embedding</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"80,-166.5 80,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"80,-189.5 135,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"135,-166.5 135,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"183.5\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 4)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"135,-189.5 232,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"183.5\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 4, 100)</text>\n</g>\n<!-- 140507752977504&#45;&gt;140507752977792 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140507752977504-&gt;140507752977792</title>\n<path fill=\"none\" stroke=\"black\" d=\"M116,-249.37C116,-241.15 116,-231.66 116,-222.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"119.5,-222.61 116,-212.61 112.5,-222.61 119.5,-222.61\"/>\n</g>\n<!-- 140507694633552 -->\n<g id=\"node3\" class=\"node\">\n<title>140507694633552</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"9,-83.5 9,-129.5 223,-129.5 223,-83.5 9,-83.5\"/>\n<text text-anchor=\"middle\" x=\"40\" y=\"-102.8\" font-family=\"Times,serif\" font-size=\"14.00\">Lambda</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"71,-83.5 71,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"98.5\" y=\"-114.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"71,-106.5 126,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"98.5\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"126,-83.5 126,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"174.5\" y=\"-114.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 4, 100)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"126,-106.5 223,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"174.5\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 100)</text>\n</g>\n<!-- 140507752977792&#45;&gt;140507694633552 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140507752977792-&gt;140507694633552</title>\n<path fill=\"none\" stroke=\"black\" d=\"M116,-166.37C116,-158.15 116,-148.66 116,-139.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"119.5,-139.61 116,-129.61 112.5,-139.61 119.5,-139.61\"/>\n</g>\n<!-- 140507694636720 -->\n<g id=\"node4\" class=\"node\">\n<title>140507694636720</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"22,-0.5 22,-46.5 210,-46.5 210,-0.5 22,-0.5\"/>\n<text text-anchor=\"middle\" x=\"47\" y=\"-19.8\" font-family=\"Times,serif\" font-size=\"14.00\">Dense</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"72,-0.5 72,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"99.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"72,-23.5 127,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"99.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"127,-0.5 127,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"168.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 100)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"127,-23.5 210,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"168.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 21)</text>\n</g>\n<!-- 140507694633552&#45;&gt;140507694636720 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140507694633552-&gt;140507694636720</title>\n<path fill=\"none\" stroke=\"black\" d=\"M116,-83.37C116,-75.15 116,-65.66 116,-56.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"119.5,-56.61 116,-46.61 112.5,-56.61 119.5,-56.61\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OBV2KdGdFGS"
      },
      "source": [
        "### Training\n",
        "\n",
        "- **Epoch** is the number of passes over the data\n",
        "- **Loss** is the error over the training set\n",
        "- The lower loss value, the better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBPFOX-JdEiQ",
        "outputId": "0b778e80-2b48-434e-fda5-a603d80c6dad"
      },
      "source": [
        "for epoch in range(1, 6):\n",
        "    loss = 0.\n",
        "    i = 0\n",
        "    for x, y in generate_context_word_pairs(corpus=padded_docs, window_size=window_size, vocab_size=vocab_size):\n",
        "        i += 1\n",
        "        loss += cbow.train_on_batch(x, y)\n",
        "        if i % 10 == 0:\n",
        "            print('Processed {} (context, word) pairs'.format(i))\n",
        "\n",
        "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 10 (context, word) pairs\n",
            "Processed 20 (context, word) pairs\n",
            "Processed 30 (context, word) pairs\n",
            "Processed 40 (context, word) pairs\n",
            "Processed 50 (context, word) pairs\n",
            "Processed 60 (context, word) pairs\n",
            "Epoch: 1 \tLoss: 190.97894859313965\n",
            "\n",
            "Processed 10 (context, word) pairs\n",
            "Processed 20 (context, word) pairs\n",
            "Processed 30 (context, word) pairs\n",
            "Processed 40 (context, word) pairs\n",
            "Processed 50 (context, word) pairs\n",
            "Processed 60 (context, word) pairs\n",
            "Epoch: 2 \tLoss: 179.77791929244995\n",
            "\n",
            "Processed 10 (context, word) pairs\n",
            "Processed 20 (context, word) pairs\n",
            "Processed 30 (context, word) pairs\n",
            "Processed 40 (context, word) pairs\n",
            "Processed 50 (context, word) pairs\n",
            "Processed 60 (context, word) pairs\n",
            "Epoch: 3 \tLoss: 166.0183641910553\n",
            "\n",
            "Processed 10 (context, word) pairs\n",
            "Processed 20 (context, word) pairs\n",
            "Processed 30 (context, word) pairs\n",
            "Processed 40 (context, word) pairs\n",
            "Processed 50 (context, word) pairs\n",
            "Processed 60 (context, word) pairs\n",
            "Epoch: 4 \tLoss: 152.13243758678436\n",
            "\n",
            "Processed 10 (context, word) pairs\n",
            "Processed 20 (context, word) pairs\n",
            "Processed 30 (context, word) pairs\n",
            "Processed 40 (context, word) pairs\n",
            "Processed 50 (context, word) pairs\n",
            "Processed 60 (context, word) pairs\n",
            "Epoch: 5 \tLoss: 142.26756384968758\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Cvhd8lgj_O"
      },
      "source": [
        "### Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "tn0jeanQhlFL",
        "outputId": "c87a3f01-3f54-4312-e105-ce10ccec6caa"
      },
      "source": [
        "import pandas as pd\n",
        "weights = cbow.get_weights()[0]\n",
        "weights = weights[1:] # notice weight starts with 1. zero was for PAD\n",
        "print(\"Matrix Shape: \", weights.shape)\n",
        "\n",
        "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix Shape:  (20, 100)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 0         1         2         3         4         5   \\\n",
              "blue      -0.015527 -0.078081 -0.089900  0.071921  0.079405  0.065083   \n",
              "beautiful -0.118906  0.034726 -0.033312  0.057658 -0.004351  0.064927   \n",
              "quick      0.043868  0.069169 -0.000574  0.020410 -0.007261  0.083199   \n",
              "brown     -0.001532 -0.100397 -0.037525 -0.000873  0.003002  0.039126   \n",
              "fox        0.014780  0.011841  0.024095  0.070992 -0.045937  0.020403   \n",
              "\n",
              "                 6         7         8         9   ...        90        91  \\\n",
              "blue       0.006259 -0.035396  0.018605 -0.023020  ... -0.068459  0.073387   \n",
              "beautiful -0.053338 -0.049812  0.125971  0.057198  ... -0.000052  0.030233   \n",
              "quick     -0.054282 -0.028413  0.006578  0.063629  ...  0.079835  0.026348   \n",
              "brown      0.113516  0.059595 -0.018083 -0.087893  ...  0.022675  0.057114   \n",
              "fox        0.028531  0.017711  0.092056 -0.081984  ... -0.107010  0.050507   \n",
              "\n",
              "                 92        93        94        95        96        97  \\\n",
              "blue      -0.065864  0.049194 -0.002877  0.043382  0.107940  0.020130   \n",
              "beautiful -0.064396  0.020565 -0.070696 -0.003824  0.032787 -0.114679   \n",
              "quick     -0.035650  0.055662 -0.056712  0.052274  0.024935  0.036396   \n",
              "brown      0.006139 -0.011332  0.049083  0.045637  0.094174  0.001736   \n",
              "fox       -0.033513 -0.011693  0.003009  0.055073 -0.005742 -0.120603   \n",
              "\n",
              "                 98        99  \n",
              "blue      -0.062042 -0.032385  \n",
              "beautiful -0.029136 -0.051736  \n",
              "quick     -0.088227 -0.047990  \n",
              "brown      0.044006 -0.048257  \n",
              "fox        0.035051 -0.071966  \n",
              "\n",
              "[5 rows x 100 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aad0f71d-7bfa-46dc-8e49-eb36ecd3932c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>blue</th>\n",
              "      <td>-0.015527</td>\n",
              "      <td>-0.078081</td>\n",
              "      <td>-0.089900</td>\n",
              "      <td>0.071921</td>\n",
              "      <td>0.079405</td>\n",
              "      <td>0.065083</td>\n",
              "      <td>0.006259</td>\n",
              "      <td>-0.035396</td>\n",
              "      <td>0.018605</td>\n",
              "      <td>-0.023020</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.068459</td>\n",
              "      <td>0.073387</td>\n",
              "      <td>-0.065864</td>\n",
              "      <td>0.049194</td>\n",
              "      <td>-0.002877</td>\n",
              "      <td>0.043382</td>\n",
              "      <td>0.107940</td>\n",
              "      <td>0.020130</td>\n",
              "      <td>-0.062042</td>\n",
              "      <td>-0.032385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beautiful</th>\n",
              "      <td>-0.118906</td>\n",
              "      <td>0.034726</td>\n",
              "      <td>-0.033312</td>\n",
              "      <td>0.057658</td>\n",
              "      <td>-0.004351</td>\n",
              "      <td>0.064927</td>\n",
              "      <td>-0.053338</td>\n",
              "      <td>-0.049812</td>\n",
              "      <td>0.125971</td>\n",
              "      <td>0.057198</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000052</td>\n",
              "      <td>0.030233</td>\n",
              "      <td>-0.064396</td>\n",
              "      <td>0.020565</td>\n",
              "      <td>-0.070696</td>\n",
              "      <td>-0.003824</td>\n",
              "      <td>0.032787</td>\n",
              "      <td>-0.114679</td>\n",
              "      <td>-0.029136</td>\n",
              "      <td>-0.051736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quick</th>\n",
              "      <td>0.043868</td>\n",
              "      <td>0.069169</td>\n",
              "      <td>-0.000574</td>\n",
              "      <td>0.020410</td>\n",
              "      <td>-0.007261</td>\n",
              "      <td>0.083199</td>\n",
              "      <td>-0.054282</td>\n",
              "      <td>-0.028413</td>\n",
              "      <td>0.006578</td>\n",
              "      <td>0.063629</td>\n",
              "      <td>...</td>\n",
              "      <td>0.079835</td>\n",
              "      <td>0.026348</td>\n",
              "      <td>-0.035650</td>\n",
              "      <td>0.055662</td>\n",
              "      <td>-0.056712</td>\n",
              "      <td>0.052274</td>\n",
              "      <td>0.024935</td>\n",
              "      <td>0.036396</td>\n",
              "      <td>-0.088227</td>\n",
              "      <td>-0.047990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brown</th>\n",
              "      <td>-0.001532</td>\n",
              "      <td>-0.100397</td>\n",
              "      <td>-0.037525</td>\n",
              "      <td>-0.000873</td>\n",
              "      <td>0.003002</td>\n",
              "      <td>0.039126</td>\n",
              "      <td>0.113516</td>\n",
              "      <td>0.059595</td>\n",
              "      <td>-0.018083</td>\n",
              "      <td>-0.087893</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022675</td>\n",
              "      <td>0.057114</td>\n",
              "      <td>0.006139</td>\n",
              "      <td>-0.011332</td>\n",
              "      <td>0.049083</td>\n",
              "      <td>0.045637</td>\n",
              "      <td>0.094174</td>\n",
              "      <td>0.001736</td>\n",
              "      <td>0.044006</td>\n",
              "      <td>-0.048257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fox</th>\n",
              "      <td>0.014780</td>\n",
              "      <td>0.011841</td>\n",
              "      <td>0.024095</td>\n",
              "      <td>0.070992</td>\n",
              "      <td>-0.045937</td>\n",
              "      <td>0.020403</td>\n",
              "      <td>0.028531</td>\n",
              "      <td>0.017711</td>\n",
              "      <td>0.092056</td>\n",
              "      <td>-0.081984</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.107010</td>\n",
              "      <td>0.050507</td>\n",
              "      <td>-0.033513</td>\n",
              "      <td>-0.011693</td>\n",
              "      <td>0.003009</td>\n",
              "      <td>0.055073</td>\n",
              "      <td>-0.005742</td>\n",
              "      <td>-0.120603</td>\n",
              "      <td>0.035051</td>\n",
              "      <td>-0.071966</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 100 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aad0f71d-7bfa-46dc-8e49-eb36ecd3932c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aad0f71d-7bfa-46dc-8e49-eb36ecd3932c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aad0f71d-7bfa-46dc-8e49-eb36ecd3932c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a matrix with similar words"
      ],
      "metadata": {
        "id": "8IUTpxyXwlYw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGsc90Kgh-gU",
        "outputId": "a69d43de-07a9-471a-e0f6-5510571a62a2"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# compute pairwise distance matrix\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "print(distance_matrix.shape)\n",
        "\n",
        "# view contextually similar words\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1]\n",
        "                   for search_term in ['quick', 'fox']}\n",
        "\n",
        "similar_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 20)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'quick': ['lazy', 'jumps', 'green', 'brown', 'beans'],\n",
              " 'fox': ['dog', 'jumps', 'brown', 'breakfast', 'green']}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize word embeddings"
      ],
      "metadata": {
        "id": "geRLdjwOwHQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.options.display.max_colwidth = 200\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "elXnpO8xwhnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
        "words_ids = [word2id[w] for w in words]\n",
        "word_vectors = np.array([weights[idx] for idx in words_ids]) # or weights[idx-1] if the error is about the index out of range\n",
        "print('Total words:', len(words), '\\tWord Embedding shapes:', word_vectors.shape)\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=3)\n",
        "np.set_printoptions(suppress=True)\n",
        "T = tsne.fit_transform(word_vectors)\n",
        "labels = words\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.scatter(T[:, 0], T[:, 1], c='steelblue', edgecolors='k')\n",
        "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "GxioX6vgwJy5",
        "outputId": "6cfe93ad-dabd-44fb-e568-2cd84c395bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words: 12 \tWord Embedding shapes: (12, 100)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = cbow.wv.index2word\n",
        "wvs = cbow.wv[words]\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=0, n_iter=5000, perplexity=2)\n",
        "np.set_printoptions(suppress=True)\n",
        "T = tsne.fit_transform(wvs)\n",
        "labels = words\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
        "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
      ],
      "metadata": {
        "id": "hZf0_PIyUi9f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}